---
title: "Students Performance Factors"
author: "Claudia Teresa Heredia Ceballos"
---

# Students Performance Factors

**Trabajo Final Data Science**

*Master en Ingenier铆a del Software: Cloud, Datos y Gesti贸n de TI*

## Descripci贸n del dominio

El rendimiento acad茅mico es un factor clave en la educaci贸n, ya que impacta directamente en las oportunidades de los estudiantes y en el desarrollo de la sociedad. Este an谩lisis busca predecir cual ser谩 el rendimiento o las calificaciones de los estudiantes seg煤n las distintas variables que los rodean en su d铆a a d铆a. 

### Enfoque del Trabajo

### Inter茅s y motivaci贸n del estudio CAMBIO

### Importancia local/nacional y en el contexto actual

```{r}
library(magrittr)
library(dplyr)
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)
```





## Descripci贸n del dataset

Para la resoluci贸n de esta pregunta se partir谩 de los datos preprocesados en 'global.rmd'.

```{r}

df_cleaned <- read.csv("data/df_cleaned.csv")
df_cleaned_c <-  df_cleaned %>%
  select(-Previous_Scores_Category)
df_cleaned_c
```



#### An谩lisis exploratorio de 'exam_score'

```{r}
# Cargar librer铆as necesarias
library(ggplot2)

# Histograma de exam_score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribuci贸n de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

```

```{r}
# Calcular estad铆sticas descriptivas
summary(df_cleaned_c$Exam_Score)

# Calcular desviaci贸n est谩ndar
sd_exam <- sd(df_cleaned_c$Exam_Score)
cat("Desviaci贸n est谩ndar de exam_score:", sd_exam)

```


## 驴Qu茅 modelo funcionar谩 mejor en la predicci贸n de exam Score?


###  Definir Variables Predictoras

Primero, se define la variable objetivo Exam_Score, que es la que se intenta predecir. Luego, se obtiene la lista de todas las variables en el DataFrame df_cleaned_c. Posteriormente, se excluyen Exam_Score, ya que es la variable objetivo, y student_id, puesto que es un identificador y no aporta informaci贸n predictiva. Finalmente, se imprime la lista de variables predictoras. 

```{r}
target_variable <- "Exam_Score"
all_variables <- names(df_cleaned_c)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)
```


###  Divisi贸n en Train/Test

Para la divisi贸n del conjutno primero, se usa set.seed(123) para asegurar la reproducibilidad del muestreo. Luego, createDataPartition() divide los datos en 80% de entrenamiento (train_data) y 20% de prueba (test_data), manteniendo la distribuci贸n de Exam_Score en ambas muestras.

```{r}
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

```



### Modelos de Machine Learning

Durante el estudio se comparar谩n 3 modelos distintos de machine learning para establecer cu谩l de ellos funciona mejor con el conjunto de datos que tenemos. 

#### Random Forest

Random Forest es 煤til porque combina m煤ltiples 谩rboles de decisi贸n, lo que mejora la precisi贸n y la capacidad de generalizaci贸n del modelo al reducir la varianza de las predicciones.

Este bloque entrena y eval煤a un modelo de Random Forest utilizando la librer铆a caret. 

* Se usa train() para entrenar el modelo y se emplea validaci贸n cruzada con 5 folds para evitar el sobreajuste y mejorar la generalizaci贸n. 

* Se habilita importance = TRUE para obtener la importancia de las variables predictoras. 

* El modelo se eval煤a haciendo predicciones en el conjunto de datos de prueba usando predict() y calculando el coeficiente de determinaci贸n 2, el cual mide qu茅 tan bien el modelo explica la variabilidad de la variable objetivo (Exam_Score). 

```{r}
set.seed(123)

rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)
```



##### Importancia de Variables en RF

```{r}
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gr谩fico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()
```


#### Regresi贸n Lineal

La regresi贸n lineal se utiliza para modelar la relaci贸n entre una variable dependiente y una o m谩s variables independientes.


```{r}
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)
```


#### SVR

Por 煤ltimo, en este apartado se entrana un modelo de Support Vector Regression. El modelo de SVR utiliza una funci贸n de n煤cleo radial (Radial Basis Function, RBF) para capturar relaciones no lineales en los datos.

En este caso tambi茅n se emplea validaci贸n cruzada con 5 folds (method = "cv", number = 5) para evitar el sobreajuste y mejorar la generalizaci贸n. Adem谩s, se preprocesan los datos centrando y escalando las variables predictoras (preProcess = c("center", "scale")) y se exploran diferentes configuraciones de hiperpar谩metros (tuneLength = 5).


```{r}

svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)
```


### Comparaci贸n de Modelos

```{r}
results_no_cluster_no_fe <- data.frame(Modelo = c("Random Forest", "Regresi贸n Lineal", "SVR"),
                              R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster_no_fe)

```




## PARTE 2 PRIMERA VERSION

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1锔 Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2锔 Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)  # Ruido moderado
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# -------------------------------
# 3锔 Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categ贸ricas a factores
df_augmented$distance_category <- as.factor(df_augmented$distance_category)
df_augmented$study_group <- as.factor(df_augmented$study_group)

# -------------------------------
# 4锔 Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5锔 Divisi贸n en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6锔 Modelos de Machine Learning
# -------------------------------
set.seed(123)

#  Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

#  Regresi贸n Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

#  SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7锔 Comparaci贸n de Modelos
# -------------------------------
results_data_augmentation <- data.frame(Modelo = c("Random Forest", "Regresi贸n Lineal", "SVR"),
                                        R2 = c(r2_rf, r2_lm, r2_svr))
print(results_data_augmentation)

# -------------------------------
# 8锔 Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gr谩fico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```







## PARTE 3 CON INTERPOLACIN

```{r}


# -------------------------------
# 1锔 Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2锔 Interpolaci贸n y generaci贸n de nuevos datos
# -------------------------------
expandir_datos <- function(df) {
  n <- nrow(df)
  
  if (n < 2) return(df)  # Evitar problemas en conjuntos peque帽os
  
  # Generar nuevos valores interpolados
  nuevos_puntos <- seq(min(df$Exam_Score), max(df$Exam_Score), length.out = n * 2)
  nuevos_puntos <- sample(nuevos_puntos, size = n, replace = TRUE)  # Aleatorizar selecci贸n
  df$Exam_Score <- nuevos_puntos
  return(df)
}

# Aplicar interpolaci贸n
df_interpolado <- expandir_datos(df_cleaned_c)

# A帽adir ruido proporcional a la dispersi贸n
df_interpolado <- df_interpolado %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_interpolado$Exam_Score) * 0.5)
)

# Limitar valores a 55-100
df_interpolado$Exam_Score <- pmax(pmin(df_interpolado$Exam_Score, 100), 55)

# -------------------------------
# 3锔 Feature Engineering
# -------------------------------
df_interpolado <- df_interpolado %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categ贸ricas a factores
df_interpolado$distance_category <- as.factor(df_interpolado$distance_category)
df_interpolado$study_group <- as.factor(df_interpolado$study_group)

# -------------------------------
# 4锔 Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_interpolado)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5锔 Divisi贸n en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_interpolado$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_interpolado[trainIndex, ]
test_data <- df_interpolado[-trainIndex, ]

# -------------------------------
# 6锔 Modelos de Machine Learning
# -------------------------------
set.seed(123)

#  Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

#  Regresi贸n Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

#  SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7锔 Comparaci贸n de Modelos
# -------------------------------
results_interpolation <- data.frame(Modelo = c("Random Forest", "Regresi贸n Lineal", "SVR"),
                                    R2 = c(r2_rf, r2_lm, r2_svr))
print(results_interpolation)

# -------------------------------
# 8锔 Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gr谩fico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()
```



## PARTE 4 CON FEATURE ENGINEERING Y SIN CLUSTERING:

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1锔 Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# Visualizar distribuci贸n de Exam_Score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribuci贸n de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

# -------------------------------
# 2锔 Feature Engineering
# -------------------------------
df_cleaned_c <- df_cleaned_c %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categ贸ricas a factores
df_cleaned_c$distance_category <- as.factor(df_cleaned_c$distance_category)
df_cleaned_c$study_group <- as.factor(df_cleaned_c$study_group)

# -------------------------------
# 3锔 Divisi贸n en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

# -------------------------------
# 4锔 Modelos de Machine Learning
# -------------------------------
set.seed(123)

#  Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

#  Regresi贸n Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

#  SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 5锔 Comparaci贸n de Modelos
# -------------------------------
results_no_cluster <- data.frame(Modelo = c("Random Forest", "Regresi贸n Lineal", "SVR"),
                                 R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster)

# -------------------------------
# 6锔 Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gr谩fico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```

## PREGUNAT 2 CON DASHBOARD

```{r}
library(shiny)
library(tidyverse)
library(caret)
library(ggplot2)
library(plotly)

# -------------------------------
# 1锔 Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)

# -------------------------------
# 2锔 Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# Imprimir desviaci贸n est谩ndar despu茅s de agregar ruido
cat("Desviaci贸n est谩ndar despu茅s de agregar ruido:", sd(df_augmented$Exam_Score), "\n")

# -------------------------------
# 3锔 Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = as.numeric(scale(Hours_Studied / (Sleep_Hours + 1))),
  motivated_study = as.numeric(scale(Hours_Studied * Motivation_Level)),
  parent_peer_influence = as.numeric(scale(Parental_Involvement * Peer_Influence)),
  distance_category = as.factor(case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  )),
  study_group = as.factor(case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  )),
  high_access_to_resources = as.factor(ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0)),
  social_influence = as.numeric(scale(Peer_Influence + Parental_Involvement + Teacher_Quality))
)

# -------------------------------
# 4锔 Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5锔 Divisi贸n en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6锔 Modelo de Regresi贸n Lineal
# -------------------------------
set.seed(123)
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)

lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# Guardar el modelo de Regresi贸n Lineal
save(lm_model, file = "best_model_lm.RData")





```


```{r}
# -------------------------------
# 7锔 Dashboard en Shiny
# -------------------------------
load("best_model_lm.RData")

# UI - Interfaz de Usuario
iu <- fluidPage(
  titlePanel("Predicci贸n de Nota en el Examen"),
  sidebarLayout(
    sidebarPanel(
      h4("Ingrese sus datos para predecir su nota"),
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          selectInput(var, var, choices = unique(df_augmented[[var]]))
        } else {
          sliderInput(var, var, min = min(df_augmented[[var]], na.rm = TRUE), max = max(df_augmented[[var]], na.rm = TRUE), value = mean(df_augmented[[var]], na.rm = TRUE))
        }
      }),
      actionButton("predict", "Predecir Nota", class = "btn-primary")
    ),
    mainPanel(
      h3("Predicci贸n de su Nota"),
      verbatimTextOutput("predicted_score")
    )
  )
)

# Server - L贸gica del Dashboard
server <- function(input, output) {
  predicted_score <- eventReactive(input$predict, {
    input_data <- data.frame(
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          as.factor(input[[var]])
        } else {
          as.numeric(input[[var]])
        }
      })
    )
    colnames(input_data) <- predictor_variables
    print(input_data)
    predict(lm_model, newdata = input_data)
  })
  
  output$predicted_score <- renderText({
    paste("Su nota estimada en el pr贸ximo examen es:", round(predicted_score(), 2))
  })
}

shinyApp(ui = iu, server = server)
```




