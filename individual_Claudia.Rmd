---
title: "Students Performance Factors"
author: "Claudia Teresa Heredia Ceballos"
---

# Students Performance Factors

**Trabajo Final Data Science**

*Master en Ingeniería del Software: Cloud, Datos y Gestión de TI*

## Descripción del dominio

El rendimiento académico es un factor clave en la educación, ya que impacta directamente en las oportunidades de los estudiantes y en el desarrollo de la sociedad. Este análisis busca predecir cual será el rendimiento o las calificaciones de los estudiantes según las distintas variables que los rodean en su día a día. 

### Enfoque del Trabajo

### Interés y motivación del estudio CAMBIO

### Importancia local/nacional y en el contexto actual

```{r}
library(magrittr)
library(dplyr)
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)
```





## Descripción del dataset

Para la resolución de esta pregunta se partirá de los datos preprocesados en 'global.rmd'.

```{r}

df_cleaned <- read.csv("data/df_cleaned.csv")
df_cleaned_c <-  df_cleaned %>%
  select(-Previous_Scores_Category)
df_cleaned_c
```



#### Análisis exploratorio de 'exam_score'

```{r}
# Cargar librerías necesarias
library(ggplot2)

# Histograma de exam_score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribución de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

```

```{r}
# Calcular estadísticas descriptivas
summary(df_cleaned_c$Exam_Score)

# Calcular desviación estándar
sd_exam <- sd(df_cleaned_c$Exam_Score)
cat("Desviación estándar de exam_score:", sd_exam)

```


## ¿Qué modelo funcionará mejor en la predicción de exam Score?


###  Definir Variables Predictoras

Primero, se define la variable objetivo Exam_Score, que es la que se intenta predecir. Luego, se obtiene la lista de todas las variables en el DataFrame df_cleaned_c. Posteriormente, se excluyen Exam_Score, ya que es la variable objetivo, y student_id, puesto que es un identificador y no aporta información predictiva. Finalmente, se imprime la lista de variables predictoras. 

```{r}
target_variable <- "Exam_Score"
all_variables <- names(df_cleaned_c)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)
```


###  División en Train/Test

Para la división del conjutno primero, se usa set.seed(123) para asegurar la reproducibilidad del muestreo. Luego, createDataPartition() divide los datos en 80% de entrenamiento (train_data) y 20% de prueba (test_data), manteniendo la distribución de Exam_Score en ambas muestras.

```{r}
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

```



### Modelos de Machine Learning

Durante el estudio se compararán 3 modelos distintos de machine learning para establecer cuál de ellos funciona mejor con el conjunto de datos que tenemos. 

#### Random Forest

Random Forest es útil porque combina múltiples árboles de decisión, lo que mejora la precisión y la capacidad de generalización del modelo al reducir la varianza de las predicciones.

Este bloque entrena y evalúa un modelo de Random Forest utilizando la librería caret. 

* Se usa train() para entrenar el modelo y se emplea validación cruzada con 5 folds para evitar el sobreajuste y mejorar la generalización. 

* Se habilita importance = TRUE para obtener la importancia de las variables predictoras. 

* El modelo se evalúa haciendo predicciones en el conjunto de datos de prueba usando predict() y calculando el coeficiente de determinación 𝑅2, el cual mide qué tan bien el modelo explica la variabilidad de la variable objetivo (Exam_Score). 

```{r}
set.seed(123)

rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)
```



##### Importancia de Variables en RF

```{r}
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()
```


#### Regresión Lineal

La regresión lineal se utiliza para modelar la relación entre una variable dependiente y una o más variables independientes.


```{r}
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)
```


#### SVR

Por último, en este apartado se entrana un modelo de Support Vector Regression. El modelo de SVR utiliza una función de núcleo radial (Radial Basis Function, RBF) para capturar relaciones no lineales en los datos.

En este caso también se emplea validación cruzada con 5 folds (method = "cv", number = 5) para evitar el sobreajuste y mejorar la generalización. Además, se preprocesan los datos centrando y escalando las variables predictoras (preProcess = c("center", "scale")) y se exploran diferentes configuraciones de hiperparámetros (tuneLength = 5).


```{r}

svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)
```


### Comparación de Modelos

```{r}
results_no_cluster_no_fe <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                              R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster_no_fe)

```




## PARTE 2 PRIMERA VERSION

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2️⃣ Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)  # Ruido moderado
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_augmented$distance_category <- as.factor(df_augmented$distance_category)
df_augmented$study_group <- as.factor(df_augmented$study_group)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7️⃣ Comparación de Modelos
# -------------------------------
results_data_augmentation <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                        R2 = c(r2_rf, r2_lm, r2_svr))
print(results_data_augmentation)

# -------------------------------
# 8️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```







## PARTE 3 CON INTERPOLACIÓN

```{r}


# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2️⃣ Interpolación y generación de nuevos datos
# -------------------------------
expandir_datos <- function(df) {
  n <- nrow(df)
  
  if (n < 2) return(df)  # Evitar problemas en conjuntos pequeños
  
  # Generar nuevos valores interpolados
  nuevos_puntos <- seq(min(df$Exam_Score), max(df$Exam_Score), length.out = n * 2)
  nuevos_puntos <- sample(nuevos_puntos, size = n, replace = TRUE)  # Aleatorizar selección
  df$Exam_Score <- nuevos_puntos
  return(df)
}

# Aplicar interpolación
df_interpolado <- expandir_datos(df_cleaned_c)

# Añadir ruido proporcional a la dispersión
df_interpolado <- df_interpolado %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_interpolado$Exam_Score) * 0.5)
)

# Limitar valores a 55-100
df_interpolado$Exam_Score <- pmax(pmin(df_interpolado$Exam_Score, 100), 55)

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_interpolado <- df_interpolado %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_interpolado$distance_category <- as.factor(df_interpolado$distance_category)
df_interpolado$study_group <- as.factor(df_interpolado$study_group)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_interpolado)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_interpolado$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_interpolado[trainIndex, ]
test_data <- df_interpolado[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7️⃣ Comparación de Modelos
# -------------------------------
results_interpolation <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                    R2 = c(r2_rf, r2_lm, r2_svr))
print(results_interpolation)

# -------------------------------
# 8️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()
```



## PARTE 4 CON FEATURE ENGINEERING Y SIN CLUSTERING:

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# Visualizar distribución de Exam_Score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribución de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

# -------------------------------
# 2️⃣ Feature Engineering
# -------------------------------
df_cleaned_c <- df_cleaned_c %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_cleaned_c$distance_category <- as.factor(df_cleaned_c$distance_category)
df_cleaned_c$study_group <- as.factor(df_cleaned_c$study_group)

# -------------------------------
# 3️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

# -------------------------------
# 4️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 5️⃣ Comparación de Modelos
# -------------------------------
results_no_cluster <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                 R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster)

# -------------------------------
# 6️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```

## PREGUNAT 2 CON DASHBOARD

```{r}
library(shiny)
library(tidyverse)
library(caret)
library(ggplot2)
library(plotly)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)

# -------------------------------
# 2️⃣ Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# Imprimir desviación estándar después de agregar ruido
cat("Desviación estándar después de agregar ruido:", sd(df_augmented$Exam_Score), "\n")

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = as.numeric(scale(Hours_Studied / (Sleep_Hours + 1))),
  motivated_study = as.numeric(scale(Hours_Studied * Motivation_Level)),
  parent_peer_influence = as.numeric(scale(Parental_Involvement * Peer_Influence)),
  distance_category = as.factor(case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  )),
  study_group = as.factor(case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  )),
  high_access_to_resources = as.factor(ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0)),
  social_influence = as.numeric(scale(Peer_Influence + Parental_Involvement + Teacher_Quality))
)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelo de Regresión Lineal
# -------------------------------
set.seed(123)
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)

lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# Guardar el modelo de Regresión Lineal
save(lm_model, file = "best_model_lm.RData")





```


```{r}
# -------------------------------
# 7️⃣ Dashboard en Shiny
# -------------------------------
load("best_model_lm.RData")

# UI - Interfaz de Usuario
iu <- fluidPage(
  titlePanel("Predicción de Nota en el Examen"),
  sidebarLayout(
    sidebarPanel(
      h4("Ingrese sus datos para predecir su nota"),
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          selectInput(var, var, choices = unique(df_augmented[[var]]))
        } else {
          sliderInput(var, var, min = min(df_augmented[[var]], na.rm = TRUE), max = max(df_augmented[[var]], na.rm = TRUE), value = mean(df_augmented[[var]], na.rm = TRUE))
        }
      }),
      actionButton("predict", "Predecir Nota", class = "btn-primary")
    ),
    mainPanel(
      h3("Predicción de su Nota"),
      verbatimTextOutput("predicted_score")
    )
  )
)

# Server - Lógica del Dashboard
server <- function(input, output) {
  predicted_score <- eventReactive(input$predict, {
    input_data <- data.frame(
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          as.factor(input[[var]])
        } else {
          as.numeric(input[[var]])
        }
      })
    )
    colnames(input_data) <- predictor_variables
    print(input_data)
    predict(lm_model, newdata = input_data)
  })
  
  output$predicted_score <- renderText({
    paste("Su nota estimada en el próximo examen es:", round(predicted_score(), 2))
  })
}

shinyApp(ui = iu, server = server)
```




