---
title: "Students Performance Factors"
author: "Claudia Teresa Heredia Ceballos"
---

# Students Performance Factors

**Trabajo Final Data Science**

*Master en Ingeniería del Software: Cloud, Datos y Gestión de TI*

## Descripción del dominio

### Enfoque del Trabajo

### Interés y motivación del estudio CAMBIO

### Importancia local/nacional y en el contexto actual

```{r}
library(magrittr)
library(dplyr)
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)
```
## PARTE 1: MODELOS SIN CLUSTERING NI FEAUTURE ENGINEERING





## Descripción del dataset

Para la resolución de esta pregunta se partirá de los datos preprocesados en 'global.rmd'.

```{r}

df_cleaned <- read.csv("data/df_cleaned.csv")
df_cleaned_c <-  df_cleaned %>%
  select(-Previous_Scores_Category)
df_cleaned_c
```

SOLUCIÓN 1: PREDICCIÓN DE EXAM_SCORE MEDIANTE GENERACIÓN DE DATOS SINTÉTICOS

#### Análisis exploratorio de 'exam_score'

```{r}
# Cargar librerías necesarias
library(ggplot2)

# Histograma de exam_score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribución de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

```

```{r}
# Calcular estadísticas descriptivas
summary(df_cleaned_c$Exam_Score)

# Calcular desviación estándar
sd_exam <- sd(df_cleaned_c$Exam_Score)
cat("Desviación estándar de exam_score:", sd_exam)

```


## PARTE 1 SIN FEAUTURE ENGINEERING NI CLUSTERING
```{r}



# -------------------------------
# 2️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_cleaned_c)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 3️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

# -------------------------------
# 4️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 5️⃣ Comparación de Modelos
# -------------------------------
results_no_cluster_no_fe <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                              R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster_no_fe)

# -------------------------------
# 6️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```

## PARTE 2 PRIMERA VERSION

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2️⃣ Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)  # Ruido moderado
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_augmented$distance_category <- as.factor(df_augmented$distance_category)
df_augmented$study_group <- as.factor(df_augmented$study_group)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7️⃣ Comparación de Modelos
# -------------------------------
results_data_augmentation <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                        R2 = c(r2_rf, r2_lm, r2_svr))
print(results_data_augmentation)

# -------------------------------
# 8️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```


## PARTE 2  Data Augmentation con Ruido Controlado





## PARTE 3 CON INTERPOLACIÓN

```{r}


# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# -------------------------------
# 2️⃣ Interpolación y generación de nuevos datos
# -------------------------------
expandir_datos <- function(df) {
  n <- nrow(df)
  
  if (n < 2) return(df)  # Evitar problemas en conjuntos pequeños
  
  # Generar nuevos valores interpolados
  nuevos_puntos <- seq(min(df$Exam_Score), max(df$Exam_Score), length.out = n * 2)
  nuevos_puntos <- sample(nuevos_puntos, size = n, replace = TRUE)  # Aleatorizar selección
  df$Exam_Score <- nuevos_puntos
  return(df)
}

# Aplicar interpolación
df_interpolado <- expandir_datos(df_cleaned_c)

# Añadir ruido proporcional a la dispersión
df_interpolado <- df_interpolado %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_interpolado$Exam_Score) * 0.5)
)

# Limitar valores a 55-100
df_interpolado$Exam_Score <- pmax(pmin(df_interpolado$Exam_Score, 100), 55)

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_interpolado <- df_interpolado %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_interpolado$distance_category <- as.factor(df_interpolado$distance_category)
df_interpolado$study_group <- as.factor(df_interpolado$study_group)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_interpolado)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_interpolado$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_interpolado[trainIndex, ]
test_data <- df_interpolado[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 7️⃣ Comparación de Modelos
# -------------------------------
results_interpolation <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                    R2 = c(r2_rf, r2_lm, r2_svr))
print(results_interpolation)

# -------------------------------
# 8️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()
```



## PARTE 4 CON FEATURE ENGINEERING Y SIN CLUSTERING:

```{r}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(ggplot2)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)  # Eliminamos una variable no utilizada

# Visualizar distribución de Exam_Score
ggplot(df_cleaned_c, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribución de 'Exam_Score'", x = "Exam Score", y = "Frecuencia")

# -------------------------------
# 2️⃣ Feature Engineering
# -------------------------------
df_cleaned_c <- df_cleaned_c %>% mutate(
  study_sleep_ratio = scale(Hours_Studied / (Sleep_Hours + 1)),
  motivated_study = scale(Hours_Studied * Motivation_Level),
  parent_peer_influence = scale(Parental_Involvement * Peer_Influence),
  distance_category = case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  ),
  study_group = case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  ),
  high_access_to_resources = ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0),
  social_influence = scale(Peer_Influence + Parental_Involvement + Teacher_Quality)
)

# Convertir variables categóricas a factores
df_cleaned_c$distance_category <- as.factor(df_cleaned_c$distance_category)
df_cleaned_c$study_group <- as.factor(df_cleaned_c$study_group)

# -------------------------------
# 3️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_cleaned_c$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_cleaned_c[trainIndex, ]
test_data <- df_cleaned_c[-trainIndex, ]

# -------------------------------
# 4️⃣ Modelos de Machine Learning
# -------------------------------
set.seed(123)

# 📌 Random Forest
rf_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)
rf_pred <- predict(rf_model, newdata = test_data)
r2_rf <- R2(rf_pred, test_data$Exam_Score)

# 📌 Regresión Lineal
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)
lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# 📌 SVR
svr_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  tuneLength = 5
)
svr_pred <- predict(svr_model, newdata = test_data)
r2_svr <- R2(svr_pred, test_data$Exam_Score)

# -------------------------------
# 5️⃣ Comparación de Modelos
# -------------------------------
results_no_cluster <- data.frame(Modelo = c("Random Forest", "Regresión Lineal", "SVR"),
                                 R2 = c(r2_rf, r2_lm, r2_svr))
print(results_no_cluster)

# -------------------------------
# 6️⃣ Importancia de Variables en RF
# -------------------------------
importance_df <- data.frame(
  Variable = rownames(importance(rf_model$finalModel)),
  Importance = importance(rf_model$finalModel)[, 1]
)
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Gráfico
ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Importancia de las Variables en Random Forest", x = "Variable", y = "Incremento en MSE") +
  theme_minimal()

```

## PREGUNAT 2 CON DASHBOARD

```{r}
library(shiny)
library(tidyverse)
library(caret)
library(ggplot2)
library(plotly)

# -------------------------------
# 1️⃣ Cargar los datos
# -------------------------------
df_cleaned <- read.csv("data/df_cleaned.csv")

df_cleaned_c <- df_cleaned %>%
  select(-Previous_Scores_Category)

# -------------------------------
# 2️⃣ Data Augmentation con Ruido Controlado
# -------------------------------
set.seed(123)
df_augmented <- df_cleaned_c %>% mutate(
  Exam_Score = Exam_Score + rnorm(n(), mean = 0, sd = sd(df_cleaned_c$Exam_Score) * 0.1)
)

# Limitar valores a 55-100
df_augmented$Exam_Score <- pmax(pmin(df_augmented$Exam_Score, 100), 55)

# Imprimir desviación estándar después de agregar ruido
cat("Desviación estándar después de agregar ruido:", sd(df_augmented$Exam_Score), "\n")

# -------------------------------
# 3️⃣ Feature Engineering
# -------------------------------
df_augmented <- df_augmented %>% mutate(
  study_sleep_ratio = as.numeric(scale(Hours_Studied / (Sleep_Hours + 1))),
  motivated_study = as.numeric(scale(Hours_Studied * Motivation_Level)),
  parent_peer_influence = as.numeric(scale(Parental_Involvement * Peer_Influence)),
  distance_category = as.factor(case_when(
    Distance_from_Home < quantile(Distance_from_Home, 0.33) ~ "Cerca",
    Distance_from_Home < quantile(Distance_from_Home, 0.66) ~ "Media",
    TRUE ~ "Lejos"
  )),
  study_group = as.factor(case_when(
    Hours_Studied < quantile(Hours_Studied, 0.33) ~ "Bajo",
    Hours_Studied < quantile(Hours_Studied, 0.66) ~ "Medio",
    TRUE ~ "Alto"
  )),
  high_access_to_resources = as.factor(ifelse(Access_to_Resources > mean(Access_to_Resources, na.rm = TRUE), 1, 0)),
  social_influence = as.numeric(scale(Peer_Influence + Parental_Involvement + Teacher_Quality))
)

# -------------------------------
# 4️⃣ Definir Variables Predictoras
# -------------------------------
target_variable <- "Exam_Score"
all_variables <- names(df_augmented)
predictor_variables <- setdiff(all_variables, c(target_variable, "student_id"))
print(predictor_variables)

# -------------------------------
# 5️⃣ División en Train/Test
# -------------------------------
set.seed(123)
trainIndex <- createDataPartition(df_augmented$Exam_Score, p = 0.8, list = FALSE)
train_data <- df_augmented[trainIndex, ]
test_data <- df_augmented[-trainIndex, ]

# -------------------------------
# 6️⃣ Modelo de Regresión Lineal
# -------------------------------
set.seed(123)
lm_model <- train(
  Exam_Score ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 5)
)

lm_pred <- predict(lm_model, newdata = test_data)
r2_lm <- R2(lm_pred, test_data$Exam_Score)

# Guardar el modelo de Regresión Lineal
save(lm_model, file = "best_model_lm.RData")





```


```{r}
# -------------------------------
# 7️⃣ Dashboard en Shiny
# -------------------------------
load("best_model_lm.RData")

# UI - Interfaz de Usuario
iu <- fluidPage(
  titlePanel("Predicción de Nota en el Examen"),
  sidebarLayout(
    sidebarPanel(
      h4("Ingrese sus datos para predecir su nota"),
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          selectInput(var, var, choices = unique(df_augmented[[var]]))
        } else {
          sliderInput(var, var, min = min(df_augmented[[var]], na.rm = TRUE), max = max(df_augmented[[var]], na.rm = TRUE), value = mean(df_augmented[[var]], na.rm = TRUE))
        }
      }),
      actionButton("predict", "Predecir Nota", class = "btn-primary")
    ),
    mainPanel(
      h3("Predicción de su Nota"),
      verbatimTextOutput("predicted_score")
    )
  )
)

# Server - Lógica del Dashboard
server <- function(input, output) {
  predicted_score <- eventReactive(input$predict, {
    input_data <- data.frame(
      lapply(predictor_variables, function(var) {
        if (var %in% c("distance_category", "study_group", "high_access_to_resources")) {
          as.factor(input[[var]])
        } else {
          as.numeric(input[[var]])
        }
      })
    )
    colnames(input_data) <- predictor_variables
    print(input_data)
    predict(lm_model, newdata = input_data)
  })
  
  output$predicted_score <- renderText({
    paste("Su nota estimada en el próximo examen es:", round(predicted_score(), 2))
  })
}

shinyApp(ui = iu, server = server)
```




